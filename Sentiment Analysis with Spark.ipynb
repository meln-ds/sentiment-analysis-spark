{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to demonstrate the use of Spark DataFrame in processing large amounts of data. I will be using a compressed version of the Yelp Academic Dataset. The dataset is not included in the github repo, but you should be able to easily download it Yelp's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('My First Spark application') \\\n",
    "    .getOrCreate() \n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 datasets in total, covering the restarant attributes, user attributes, checkins, reviews and tips. You can get details about each dataframe by calling .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "business = spark.read.json('../non_auto_assignments/data/yelp_academic/yelp_academic_dataset_business.json.gz')\n",
    "checkin = spark.read.json('../non_auto_assignments/data/yelp_academic/yelp_academic_dataset_checkin.json.gz')\n",
    "review = spark.read.json('../non_auto_assignments/data/yelp_academic/yelp_academic_dataset_review.json.gz')\n",
    "tip = spark.read.json('../non_auto_assignments/data/yelp_academic/yelp_academic_dataset_tip.json.gz')\n",
    "user = spark.read.json('../non_auto_assignments/data/yelp_academic/yelp_academic_dataset_user.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: string (nullable = true)\n",
      " |-- attributes: struct (nullable = true)\n",
      " |    |-- AcceptsInsurance: string (nullable = true)\n",
      " |    |-- AgesAllowed: string (nullable = true)\n",
      " |    |-- Alcohol: string (nullable = true)\n",
      " |    |-- Ambience: string (nullable = true)\n",
      " |    |-- BYOB: string (nullable = true)\n",
      " |    |-- BYOBCorkage: string (nullable = true)\n",
      " |    |-- BestNights: string (nullable = true)\n",
      " |    |-- BikeParking: string (nullable = true)\n",
      " |    |-- BusinessAcceptsBitcoin: string (nullable = true)\n",
      " |    |-- BusinessAcceptsCreditCards: string (nullable = true)\n",
      " |    |-- BusinessParking: string (nullable = true)\n",
      " |    |-- ByAppointmentOnly: string (nullable = true)\n",
      " |    |-- Caters: string (nullable = true)\n",
      " |    |-- CoatCheck: string (nullable = true)\n",
      " |    |-- Corkage: string (nullable = true)\n",
      " |    |-- DietaryRestrictions: string (nullable = true)\n",
      " |    |-- DogsAllowed: string (nullable = true)\n",
      " |    |-- DriveThru: string (nullable = true)\n",
      " |    |-- GoodForDancing: string (nullable = true)\n",
      " |    |-- GoodForKids: string (nullable = true)\n",
      " |    |-- GoodForMeal: string (nullable = true)\n",
      " |    |-- HairSpecializesIn: string (nullable = true)\n",
      " |    |-- HappyHour: string (nullable = true)\n",
      " |    |-- HasTV: string (nullable = true)\n",
      " |    |-- Music: string (nullable = true)\n",
      " |    |-- NoiseLevel: string (nullable = true)\n",
      " |    |-- Open24Hours: string (nullable = true)\n",
      " |    |-- OutdoorSeating: string (nullable = true)\n",
      " |    |-- RestaurantsAttire: string (nullable = true)\n",
      " |    |-- RestaurantsCounterService: string (nullable = true)\n",
      " |    |-- RestaurantsDelivery: string (nullable = true)\n",
      " |    |-- RestaurantsGoodForGroups: string (nullable = true)\n",
      " |    |-- RestaurantsPriceRange2: string (nullable = true)\n",
      " |    |-- RestaurantsReservations: string (nullable = true)\n",
      " |    |-- RestaurantsTableService: string (nullable = true)\n",
      " |    |-- RestaurantsTakeOut: string (nullable = true)\n",
      " |    |-- Smoking: string (nullable = true)\n",
      " |    |-- WheelchairAccessible: string (nullable = true)\n",
      " |    |-- WiFi: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- hours: struct (nullable = true)\n",
      " |    |-- Friday: string (nullable = true)\n",
      " |    |-- Monday: string (nullable = true)\n",
      " |    |-- Saturday: string (nullable = true)\n",
      " |    |-- Sunday: string (nullable = true)\n",
      " |    |-- Thursday: string (nullable = true)\n",
      " |    |-- Tuesday: string (nullable = true)\n",
      " |    |-- Wednesday: string (nullable = true)\n",
      " |-- is_open: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the business dataset\n",
    "business.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- average_stars: double (nullable = true)\n",
      " |-- compliment_cool: long (nullable = true)\n",
      " |-- compliment_cute: long (nullable = true)\n",
      " |-- compliment_funny: long (nullable = true)\n",
      " |-- compliment_hot: long (nullable = true)\n",
      " |-- compliment_list: long (nullable = true)\n",
      " |-- compliment_more: long (nullable = true)\n",
      " |-- compliment_note: long (nullable = true)\n",
      " |-- compliment_photos: long (nullable = true)\n",
      " |-- compliment_plain: long (nullable = true)\n",
      " |-- compliment_profile: long (nullable = true)\n",
      " |-- compliment_writer: long (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- elite: string (nullable = true)\n",
      " |-- fans: long (nullable = true)\n",
      " |-- friends: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- yelping_since: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the user dataset\n",
    "user.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- compliment_count: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the tip dataset\n",
    "tip.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis With Restaurant Reviews\n",
    "\n",
    "\n",
    "We all know that reading user reviews is a reliable measure when choosing whether to dine at a particular restaurant. But what exactly constitutes a positive, or a negative reviews ? Yelp has done a great job embedding sentiments into reviews by allowing users to 'react' to other users' reviews. Which brings me to the next question -- How do we know when a review is 'funny', or 'cool' ? Let's find out.\n",
    "\n",
    "First, let's get familiarized with spark by answering a few basic questions. How many users have received a whole lot of cool compliments (> 5000) ? By reading the schema of the datasets, we know that the answer lies in the user dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(average_stars=4.03, compliment_cool=1, compliment_cute=0, compliment_funny=1, compliment_hot=2, compliment_list=0, compliment_more=0, compliment_note=1, compliment_photos=0, compliment_plain=1, compliment_profile=0, compliment_writer=2, cool=25, elite='2015,2016,2017', fans=5, friends='c78V-rj8NQcQjOI8KP3UEA, alRMgPcngYSCJ5naFRBz5g, ajcnq75Z5xxkvUSmmJ1bCg, BSMAmp2-wMzCkhTfq9ToNg, jka10dk9ygX76hJG0gfPZQ, dut0e4xvme7QSlesOycHQA, l4l5lBnK356zBua7B-UJ6Q, 0HicMOOs-M_gl2eO-zES4Q, _uI57wL2fLyftrcSFpfSGQ, T4_Qd0YWbC3co6WSMw4vxg, iBRoLWPtWmsI1kdbE9ORSA, xjrUcid6Ymq0DoTJELkYyw, GqadWVzJ6At-vgLzK_SKgA, DvB13VJBmSnbFXBVBsKmDA, vRP9nQkYTeNioDjtxZlVhg, gT0A1iN3eeQ8EMAjJhwQtw, 6yCWjFPtp_AD4x93WAwmnw, 1dKzpNnib-JlViKv8_Gt5g, 3Bv4_JxHXq-gVLOxYMQX0Q, ikQyfu1iViYh8T0us7wiFQ, f1GGltNaB7K5DR1jf3dOmg, tgeFUChlh7v8bZFVl2-hjQ, -9-9oyXlqsMG2he5xIWdLQ, Adj9fBPVJad8vSs-mIP7gw, Ce49RY8CKXVsTifxRYFTsw, M1_7TLi8CbdA89nFLlH4iw, wFsNv-hqbW_F5-IRqfBN6g, 0Q1L7zXHocaUZ2gsG2XJeg, cBFgmOCBdhYa0xoFEAzp_g, VrD_AgiFvzqtlR15vir3SQ, cpE-7HK514Sr5vpSen9CEQ, F1UYelhPFB-zIKlt0ygIZg, CQAL1hvsLMCzuJf9AglsXw, 1KnY1wr15WfEWIRLB9IS6g, QWFQ-kXBiLbid-lm5Jr3dQ, nymT8liFugCrM16lTy0ZfQ, qj69bdd885heDvUPCyHd2Q, DySCZZcgbdrlHgEovk5y9w, lZMJIDuvhT9Dy4KyquLXyA, b_9Gn7wS93AoPZPR0dIJqQ, N07g1IaLh0_6sUjtiSRe4w, YdfPX_7DxSnKvvdCJ57iOw, 8GYryZPD22W7WgQ8kvMkEQ, cpQmAgOWatghp14h1pn1dQ, EnchhymLYMqftCRjqvVWmw, -JdfKhFktE7Zs9BMDFcPeQ, uWhC9eof98zPkvsalgaqJw, eyTlNDDaiPatfe6mheIZ0g, VfHq0o73aKsODvfAhwAQtg, kvD5tICngLAaQDujSFWupA, dXacwEhqi9-3_XT6JeH0Og, NfU0zDaTMEQ4-X9dbQWd9A, cTHWBdjSKbctSUIvWsgFxw, 3IEtCbSDF5t7RkZ20T6s9A, HJJXTrp6UybYyPdQ9DA0JA, JaXogQFVjzGRAeBvzamBHg, NUonfKkjS1iVqnNITtgXZg, D5vaJAYp0sOrGfsj9qvsMA, H27Ecbwwu4FGAlLgICourw, S8HrLmMiE4u8FWYWkNEoTw, Io36Y3xWQcIX9rYvPcYfXQ, J5mcqh8KxYpqjaLBNlwcig, -nTB3_08g06fD0GT8AtDBQ, wMpFA46lihK8oFns_5p65A, RZGFJHeomGJCWp3xcL3ejA, ZoQSzzXoSP1RxOD4Amv9Bg, qzM0EB0SkuuGIFv0adjQAQ, HuM6vvuveken-fPZ7d4olA, H3oukHpGpn9n_mJwSDSQyQ, PkmsJsQ8FIZe8eh8c_u96g, wSByVbwME4MzgkJaFyfvNg, YEVqknoDmrHAoUbHX0nPnA, li3vsK1XAPmeJYAUTYflHQ, MKc8yXi0glbPYt0Qb4PECw, fQPH6W9fksi27gkuUPnFaA, amrCMrDsoRetYFg2kwwdFA, 84dVQ6n6r2ezNaTuc7RkKA, yW9QjWY0olv5-uRKv3t_Kw, 5XJDj7c3eoidfQ3jW18Zgw, txSc6a6pIDctvwyBeu7Aqg, HFbbDCyyqP9xPkUlcxeIdg, hTUv5oh2do6Z3OppPuuiJA, gSqonG9J4fNM-fl_fE71AA, pd9mgTFpBTg5F9x-MsczNg, j3VE22V2GcHiH8UZxfFLfw, NYXlMW-T-3V4Jqr4r-i0Wg, btxgAZedxX8IWhMifA7Xkg, -Hp5mPLiRJNFnyeX5Ygzag, P6-DwVg6-t2JuQwIUEk0iQ, OI2TvxYvZrAodBG_RF53Xw, bHxf_VPKmZur1Bier-6A2A, Et_Sb39cVm81_Xe9HDM8ZQ, 5HwGl2UyYbaRq8aD6YC-fA, ZK228WMcCKLo5thcjD7rdw, iTf8wojwfm0NOi7dOiz3Nw, btYRxQYNJjpecflNHtFH0A, Kgo42FzpW_dXFgDKoewbtg, MNk_1Q_dqOY3xxHZKeO8VQ, AlwD504T9k0m5lkg3k5g6Q', funny=17, name='Rashmi', review_count=95, useful=84, user_id='l6BmjZMeQD3rDxWUbiAiow', yelping_since='2013-10-08 23:11:33')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print the first line of the user df out\n",
    "user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the number of users with more than 5000 compliments, the process is:\n",
    "- Filter the dataframe to only users with more than 5000 compliments.\n",
    "- Use collect() to get the filtered results in a Python list.\n",
    "- Use len() to get the number of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User with more than 5000 compliments\n",
    "\n",
    "len(user.filter(user['compliment_cool'] > 5000).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's have a look at the businesses. What are our top complimented businesses ? To find out, we will need to look at the tip dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('business_id', 'string'),\n",
       " ('compliment_count', 'bigint'),\n",
       " ('date', 'string'),\n",
       " ('text', 'string'),\n",
       " ('user_id', 'string')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the datatypes in the tip df\n",
    "tip.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tip df does contain information to compliment counts, but does not contain the business names. To get them, we'll first need to join with the business df on business_id. The next step is to group by name and sum the number of compliments. Sort the df so that the top 10 business names are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|                name|total_compliment|\n",
      "+--------------------+----------------+\n",
      "|           Starbucks|             213|\n",
      "|Il Chianti Italia...|              68|\n",
      "|          McDonald's|              62|\n",
      "|McCarran Internat...|              54|\n",
      "|    Bacchanal Buffet|              50|\n",
      "|     Costco Gasoline|              45|\n",
      "| Walmart Supercenter|              43|\n",
      "|     In-N-Out Burger|              43|\n",
      "|        Trader Joe's|              41|\n",
      "|Chipotle Mexican ...|              40|\n",
      "+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = tip \\\n",
    ".join(business, tip.business_id == business.business_id) \\\n",
    ".groupby('name') \\\n",
    ".agg(func.sum(\"compliment_count\") \\\n",
    "     .alias('total_compliment')) \\\n",
    ".orderBy('total_compliment', ascending=False) \\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly a lot of the big names show up, like Starbucks or MacD. It's also interesting to note that 8/10 businesses are restaurants, with the exception of Costco Gasoline and Walmart. I wonder if I would ever leave a review for a supermarket! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at the reviews. What are the top 10 useful positive reviews ? For this question, we'll take a look at the review df. Note that the reviews should be **positive** reviews. Only reviews scoring 3 stars and above will be considered positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the review dataset\n",
    "review.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(business_id='ujmEBvifdJM6h6RLv4wQIg', cool=0, date='2013-05-07 04:34:36', funny=1, review_id='Q1sbwvVQXV2734tPgoKj4Q', stars=1.0, text='Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge us $69 for 3 pills. I checked online the pills can be had for 19 cents EACH! Avoid Hospital ERs at all costs.', useful=6, user_id='hG7b0MtEbXx5QzbzE6C_VA')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the first row\n",
    "review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result can be obtained by first filtering out positive (3 and above stars) reviews, then group by the review ids and sum the number of usefulness counts. Sort the result in a descending order and output the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|           review_id|most_useful|\n",
      "+--------------------+-----------+\n",
      "|O1YX1g7Wbf0rmcoud...|        808|\n",
      "|1lGXlyq4MALOMx17v...|        358|\n",
      "|7BNr0xFRpOO4PKvbv...|        333|\n",
      "|RfqGVszoXCw5YhDxj...|        303|\n",
      "|gAUkgn4dTO-R2n5LB...|        278|\n",
      "|5S985RjfmDJYsJvUt...|        244|\n",
      "|0nr6SQFKpR6JCYl1z...|        241|\n",
      "|-hRpmcavsC0UDI_Qs...|        235|\n",
      "|ScbDdrWZLmPdHDD1-...|        229|\n",
      "|UlAVHI58m5XLWvzzh...|        228|\n",
      "+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review \\\n",
    ".filter(review['stars'] >= 3.0) \\\n",
    ".groupby('review_id') \\\n",
    ".agg(func.sum(\"useful\") \\\n",
    "     .alias('most_useful')) \\\n",
    ".orderBy('most_useful', ascending=False) \\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That isn't particularly useful (ironically), but bear with me here. We will be using this for the sentiment analysis in just a bit. Before we get there, let's try to look at one more thing -- During what hour of the day do most checkins occur? As the question suggests, we shall be inspecting the checkin dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkin.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(business_id='--1UhMGODdWsrMastO9DZw', date='2016-04-26 19:49:16, 2016-08-30 18:36:57, 2016-10-15 02:45:18, 2016-11-18 01:54:50, 2017-04-20 18:39:06, 2017-05-03 17:58:02')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll split the string of dates into individual dates, similar to what was done in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, explode, hour, to_timestamp\n",
    "from pyspark.sql.types import ArrayType,StringType\n",
    "datesplit = udf(lambda x: x.split(','), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_time = checkin \\\n",
    ".select('business_id', datesplit('date').alias('dates')) \\\n",
    ".withColumn('checkin_date', explode('dates'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert the date string into timestamp, and extract the hour value from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_time_converted = checkin_time \\\n",
    ".select('business_id', 'dates', 'checkin_date') \\\n",
    ".withColumn('hour', hour(to_timestamp('checkin_date', 'yyyy-MM-dd HH:mm:ss')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(business_id='--1UhMGODdWsrMastO9DZw', dates=['2016-04-26 19:49:16', ' 2016-08-30 18:36:57', ' 2016-10-15 02:45:18', ' 2016-11-18 01:54:50', ' 2017-04-20 18:39:06', ' 2017-05-03 17:58:02'], checkin_date='2016-04-26 19:49:16', hour=19)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_time_converted.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to group by the hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|hour|checkin_count|\n",
      "+----+-------------+\n",
      "|   1|      1561788|\n",
      "|  19|      1502271|\n",
      "|   0|      1491176|\n",
      "|   2|      1411255|\n",
      "|  20|      1350195|\n",
      "+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkin_time_converted \\\n",
    ".groupby('hour') \\\n",
    ".agg(func.count(\"business_id\") \\\n",
    "     .alias('checkin_count')) \\\n",
    ".orderBy('checkin_count', ascending=False) \\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most check-ins occur at 1am in the morning, followed by 7pm in the evening and midnight. It is rather surprising see how many checkins are done at and post-midnight. However the dataset does not give us information on whether all checkins are in the same timezone and the timezone difference if any could potentially be an impact in how we get the number of checkins by hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now to the star of the show - which are the non-stopwords that we will see the most in a positive, and vice versa, in a negative review?** Having this insight would be extremely useful in allowing us to predict sentiment in a review later on, say in case the reviews do not come with any sentiment attributes like this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider the following two reviews:\n",
    "\n",
    "* Positive: The meal was great, and the service was the best we ever experienced.\n",
    "* Negative: The meal was awful.  It was the worst thing we ever experienced.\n",
    "\n",
    "Assume our stopwords are {'the','was','and','the','was','we','it'}\n",
    "\n",
    "* Positive unique: {'great', 'service', 'best'}\n",
    "\n",
    "* Negative unique: {'awful', 'worst', 'thing'}\n",
    "\n",
    "In this example, each unique word occurs just once, so the concept of \"top 50\" doesn't make sense.  For your data, you'll need to count the number of times each unique word occurs. Using the same definition as what was laid out previously, a review is considered positive if it has 3 stars and above. Vice versa, a review is negative if it has less than 3 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of stopwords\n",
    "STOPWORDS_AND_SPACE = ['', 'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 separate df for positive and negative\n",
    "pos = review.filter(review['stars'] >= 3.0)\n",
    "neg = review.filter(review['stars'] < 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is, for each positive/negative dataframe, get all the unique words within that dataframe and their corresponding counts. This can be done with the following steps:\n",
    "- Create a list of stopwords. Note that I also included whitespace for a regex split later on\n",
    "- Split the review text into a list of words, and use explode() to create separate rows for each word.\n",
    "- Group the dataFrame by the word and take the count.\n",
    "- Sort the count in a descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the review string annd strip out all the stopwords \n",
    "import re\n",
    "stringsplit = udf(lambda line: [w for w in re.split(\"[^A-Za-z0-9]\", line.strip().lower()) if w not in STOPWORDS_AND_SPACE], ArrayType(StringType()))\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_split = pos \\\n",
    ".select('review_id', stringsplit('text').alias('words')) \\\n",
    ".withColumn('word', explode('words'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+\n",
      "|           review_id|               words|    word|\n",
      "+--------------------+--------------------+--------+\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|   adore|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|  travis|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|    hard|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|    rock|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|     new|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|   kelly|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|cardenas|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|   salon|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|       m|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|  always|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|     fan|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|   great|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...| blowout|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|stranger|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|  chains|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|   offer|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...| service|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...| however|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|  travis|\n",
      "|GJXCdrto3ASJOqKeV...|[adore, travis, h...|   taken|\n",
      "+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_split.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words_count = pos_split \\\n",
    ".groupby('word') \\\n",
    ".agg(func.count('review_id') \\\n",
    "     .alias('word_count')) \\\n",
    ".orderBy('word_count', ascending=False)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same for the negative dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_split = neg \\\n",
    ".select('review_id', stringsplit('text').alias('words')) \\\n",
    ".withColumn('word', explode('words'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+\n",
      "|           review_id|               words|    word|\n",
      "+--------------------+--------------------+--------+\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|   total|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|    bill|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|horrible|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...| service|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|     8gs|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|  crooks|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|actually|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|   nerve|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|  charge|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|      us|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|      69|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|       3|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|   pills|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...| checked|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|  online|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|   pills|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|      19|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|   cents|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|   avoid|\n",
      "|Q1sbwvVQXV2734tPg...|[total, bill, hor...|hospital|\n",
      "+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neg_split.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words_count = neg_split \\\n",
    ".groupby('word') \\\n",
    ".agg(func.count('review_id') \\\n",
    "     .alias('word_count')) \\\n",
    ".orderBy('word_count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above has all the unique words and their counts, but has not addressed whether a word is only unique to positive reviews, i.e. a word appears only in the positive df, but not the negative df, and vice versa. To do this, we can leverage **Spark's left anti-join.** A left anti-join will return rows that are only in the first dataframe, but not in the second dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                word|word_count|\n",
      "+--------------------+----------+\n",
      "|               eloff|       302|\n",
      "|                jabz|       202|\n",
      "|              fixler|       168|\n",
      "|         ahhhhmazing|       167|\n",
      "|              popbar|       146|\n",
      "|           heartwood|       143|\n",
      "|                emme|       137|\n",
      "|              boothe|       134|\n",
      "|               artur|       126|\n",
      "|           delizioso|       124|\n",
      "|                homa|       123|\n",
      "|               safak|       119|\n",
      "|                 f45|       117|\n",
      "|              sidell|       114|\n",
      "|        shutterbooth|       110|\n",
      "|                meux|       107|\n",
      "|y3fcl4bly0ellkb0s...|       106|\n",
      "|               hubba|       105|\n",
      "|            perfects|       104|\n",
      "|              exquis|       101|\n",
      "|                 imr|       100|\n",
      "|           merveille|        98|\n",
      "|             hobgood|        98|\n",
      "|             koshari|        97|\n",
      "|           ahhmazing|        95|\n",
      "|               trego|        89|\n",
      "|              omalza|        87|\n",
      "|       scrumptiously|        87|\n",
      "|             wooffle|        87|\n",
      "|          deeeeelish|        86|\n",
      "|              hiroba|        86|\n",
      "|             kikuchi|        85|\n",
      "|              winnah|        83|\n",
      "|             skaught|        82|\n",
      "|               saura|        82|\n",
      "|                etti|        82|\n",
      "|                 meu|        81|\n",
      "|               sirna|        81|\n",
      "|              saskia|        81|\n",
      "|               feutr|        81|\n",
      "|              kozlik|        80|\n",
      "|               hiers|        79|\n",
      "|              kismet|        79|\n",
      "|               combl|        78|\n",
      "|         succulentes|        76|\n",
      "|            cranmore|        76|\n",
      "|             cadeaux|        76|\n",
      "|no5e0wtlaoycjhelu...|        75|\n",
      "|               yumma|        75|\n",
      "|             bombbbb|        75|\n",
      "+--------------------+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_unique = pos_words_count \\\n",
    ".join(neg_words_count, on=['word'], how='left_anti') \\\n",
    ".orderBy('word_count', ascending=False) \\\n",
    ".show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+\n",
      "|           word|word_count|\n",
      "+---------------+----------+\n",
      "|   ripoffreport|        72|\n",
      "|consumeraffairs|        41|\n",
      "|   unempathetic|        38|\n",
      "|      discusted|        30|\n",
      "|           fahw|        30|\n",
      "|         voiers|        30|\n",
      "|     unlawfully|        28|\n",
      "|     nonpayment|        27|\n",
      "|            ybt|        26|\n",
      "|            amj|        26|\n",
      "|         theifs|        26|\n",
      "|            hra|        23|\n",
      "|           azag|        23|\n",
      "|           tkps|        23|\n",
      "|        horable|        22|\n",
      "|       comenity|        22|\n",
      "|        frauded|        21|\n",
      "|       cobraron|        20|\n",
      "|        suppost|        20|\n",
      "|          nedia|        20|\n",
      "|       belitsky|        19|\n",
      "|      fraudster|        19|\n",
      "|   incompetance|        19|\n",
      "|            1ns|        19|\n",
      "|        searshc|        19|\n",
      "|          fmcsa|        19|\n",
      "|      repossess|        19|\n",
      "|         rebill|        19|\n",
      "|          emele|        18|\n",
      "|       thummala|        18|\n",
      "|          azdhs|        17|\n",
      "|          usaco|        16|\n",
      "|      colquitte|        16|\n",
      "|     gutterdome|        16|\n",
      "|     vstorewtfe|        16|\n",
      "|       contrato|        16|\n",
      "| pissedconsumer|        16|\n",
      "|      donotcall|        15|\n",
      "|        struggs|        15|\n",
      "|          cdiff|        15|\n",
      "|          egift|        15|\n",
      "|     capitalone|        15|\n",
      "|         roachs|        15|\n",
      "|         denyed|        15|\n",
      "|         zeneba|        15|\n",
      "|         nuanez|        15|\n",
      "|      onetravel|        15|\n",
      "|    uncollected|        15|\n",
      "|       teribble|        14|\n",
      "|        amhurst|        14|\n",
      "+---------------+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same with negative\n",
    "neg_unique = neg_words_count \\\n",
    ".join(pos_words_count, on=['word'], how='left_anti') \\\n",
    ".orderBy('word_count', ascending=False) \\\n",
    ".show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is astonishing. For example, check out the most frequent word in the positive review bucket: **\"eloff\"**. Does it ring a bell to you? Not so much. And yet for some reason, this word reigned champion. It is also interesting to note that because these reviews are written by humans, it is not so surprising to see words like **\"ahhhhmazing\"** (heck, I do that a lot myself). Same goes with the negative reviews' results, where we can see words like **\"discusted\"** coming up on top. I guess typos aren't so important when you are writing an angry review :)\n",
    "\n",
    "This is an important consideration when constructing a library of words and their associated sentiments. Not all words are grammatically accurate, and yet contains important information about how a user feels about a business! \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
